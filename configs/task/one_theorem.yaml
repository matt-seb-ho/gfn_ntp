name: "ntp"

data:
  path: null
  train_size: 1
  train_data_path: "data/single_thm.json"
  val_data_path: "data/single_thm.json"
  repeat_train_theorems: 1

model:
  # test with llemma, run later with dsp
  # hf_id: "EleutherAI/llemma_7b"
  # hf_id: "deepseek-ai/DeepSeek-Prover-V1"
  hf_id: "kaiyuy/leandojo-lean4-tacgen-byt5-small"
  seq2seq: true 
  use_lora: false
  # seq2seq: false

  # specify a huggingface model id to initialize the policy adapter from
  # initial_policy_adapter: "msho/dspv1_policy_sft"
  initial_policy_adapter: null

  bnb:
    _target_: transformers.BitsAndBytesConfig
    load_in_4bit: true
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: bfloat16

  lora_config:
    _target_: peft.LoraConfig

    # adjusted settings for OOM issues
    target_modules: "all-linear"
    r: 32
    lora_alpha: 32
    lora_dropout: 0.05
    bias: "none"
    # task_type
    # - SEQ_2_SEQ_LM: Sequence-to-sequence language modeling.
    # - CAUSAL_LM: Causal language modeling.
    task_type: "SEQ_2_SEQ_LM"
    use_rslora: true

    # settings from SFT RM config
    # - https://www.philschmid.de/fine-tune-llms-in-2024-with-trl
    # target_modules: "all-linear"
    # r: 256
    # lora_alpha: 128
    # lora_dropout: 0.05
    # bias: "none"
    # task_type: "CAUSAL_LM"

    # next_sentence settings (gpt-2xl)
    # target_modules: ["c_attn", "c_proj", "c_fc"]
    # r: 64
    # lora_alpha: 16
    # lora_dropout: 0.1
    # bias: "none"
    # fan_in_fan_out: true
  
training:
  # subtb_lambda: 1.0
  # from gfn+llm paper:
  # pf_temp_high: 2.0
  # pf_temp_low: 0.5
  pf_temp_high: 1.0
  pf_temp_low: 0.25
  pf_temp_prob: 0.666
  # use_buffer_prob: 1 # guarantees buffer usage (offline training)
  use_buffer_prob: 0 # guarantees online training
  n_samples: 4 # next_sentence task had 20
  branch_only_at_root: true
  lr: 0.001
  accumulate_grad_batches: 1 # TESTING!
  epochs: 200 # next_sentence task had 300
  use_4bit: false
  dojo_timeout: 60 # in seconds (LeanDojo's default is 600)
  ckpt_dest: "checkpoints"
  save_ckpt_on_val: true
  gradient_clip_val: 0.5 # default is 0 (no clipping)
  num_sanity_val_steps: 0
  replay_batch_size: 4
  truncate_state: true
  
  log_every_n_steps: 1
  # validation loop frequency control
  # val_check_interval: float in [0, 1]: frequency per train epoch
  val_check_interval: null
  # check_val_every_n_epoch: int (default=1)
  check_val_every_n_epoch: 10

  # whether to learn the log_z prediction head or unconditional parameter
  conditional_log_z: true

reward:
  temp_start: 1.0
  temp_end: 1.0
  temp_horizon: 750
  vocab_alpha: -50
  sentence_validator: null
  buffer_size: 50
  buffer_sim_tolerance: 0.25
  # buffer_seed_trajectory_file: "data/single_theorem_seed_trajectories.json"
  # buffer_seed_trajectory_file: "data/t0_seed_gtt_only.json"
  buffer_seed_trajectory_file: data/t0_seed_t_updated_fmt.json
  verifier_batch_size: 6 # TODO: figure out if we can go higher
  use_sts_format: false
  error_length_penalty_a: 8.0
  
  
  # reward model settings
  model:
    # setup options: null, base, adapter, independent
    setup: null
    # setup: adapter # previous setup for dspv1
    adapter:
      # hf_id: "msho/llemma_sft"
      # hf_id: "msho/dspv1_sft"
      # name: "reward"
      hf_id: null
      name: null
    # following settings are for setup="independent", 
    # where RM does not share weights with the policy
    hf_id: null
    peft: false
    seq2seq: true
    share_tokenizer: true

callbacks:
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/logR"
    mode: "max"
    save_last: true
    dirpath: ${save_dir}/checkpoints/${now:%Y-%m-%d}_${now:%H-%M-%S}
    filename: "epoch={epoch:03d}"
    auto_insert_metric_name: true
  # don't use early stopping for now
  # - _target_: pytorch_lightning.callbacks.EarlyStopping
  #   monitor: "val/logR"
  #   mode: "max"
  #   patience: 10

  # next_sentence settings:
  # - _target_: pytorch_lightning.callbacks.ModelCheckpoint
  #   monitor: "val/logP(s) (avg)"
  #   mode: "max"
  #   save_last: true
  #   dirpath: ${save_dir}/checkpoints/${now:%Y-%m-%d}_${now:%H-%M-%S}
  #   filename: "epoch={epoch:03d}"
  #   auto_insert_metric_name: true
  # - _target_: pytorch_lightning.callbacks.EarlyStopping
  #   monitor: "val/logP(s) (avg)"
  #   mode: "max"
  #   patience: 10

constraints:
  max_tactics: 3
  # min_tactic_tokens: 1
  min_tactic_tokens: 3
  # max_tactic_tokens: 30
  max_tactic_tokens: 87
  max_input_length: 900

debug_logger:
  include_lean_dojo_debug: true
  log_debug_to_stdout: false
  write_to_file: true
  debug_log_file: "logs/single_thm_sancheck.log"
  
search_eval:
  probe_count: null # null for all
  sanity_check_probe_count: 0
  probe_file: data/single_thm.json

  search_params:
    _target_: proof_flow.src.search.common.ProofSearchParams
    num_sampled_tactics: 8
    timeout: 30
    max_expansions: null
    max_depth: 6
    num_workers: 1
    num_gpus: 1
    # max_input_seq_len: 230 # 200 (state) + 30 (prompt)
    max_input_seq_len: 900 # reprover (max state is 265)
    max_output_seq_len: 987
    # max_new_tokens: 30
    max_new_tokens: 87 # reprover
    length_penalty: 0.0

prompts:
  # tac_gen: "ds_rm_st_v2"
  tac_gen: "rp_tacgen" # reprover template ("{state}")
  reward: "reprover"

# ground truth trajectories
gtt:
  file_path: data/one_theorem_gtt.json
  # file_path: null
  write_to_file: true
  seed_replay_buffer: false
