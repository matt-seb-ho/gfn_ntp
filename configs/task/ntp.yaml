name: "ntp"

data:
  path: "data/long1k_train.json"
  train_size: 0.95
  train_data_path: null
  val_data_path: null

model:
  # test with llemma, run later with dsp
  # name: "EleutherAI/llemma_7b"
  name: "deepseek-ai/DeepSeek-Prover-V1"

  bnb:
    _target_: transformers.BitsAndBytesConfig
    load_in_4bit: true
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: bfloat16

  lora_config:
    _target_: peft.LoraConfig

    # adjusted settings for OOM issues
    target_modules: "all-linear"
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"
    use_rslora: true

    # settings from SFT RM config
    # - https://www.philschmid.de/fine-tune-llms-in-2024-with-trl
    # target_modules: "all-linear"
    # r: 256
    # lora_alpha: 128
    # lora_dropout: 0.05
    # bias: "none"
    # task_type: "CAUSAL_LM"

    # next_sentence settings (gpt-2xl)
    # target_modules: ["c_attn", "c_proj", "c_fc"]
    # r: 64
    # lora_alpha: 16
    # lora_dropout: 0.1
    # bias: "none"
    # fan_in_fan_out: true
  
  # specify a huggingface model id to initialize the policy adapter from
  # e.g. "msho/llemma_sft"
  initialize_policy_adapter_from_pretrained: "msho/dspv1_policy_sft"

  # currently just used for replay
  inf_batch_size: 8

training:
  # subtb_lambda: 1.0
  pf_temp_high: 2.0
  pf_temp_low: 0.5
  pf_temp_prob: 0.666
  use_buffer_prob: 0.25
  n_samples: 4  # next_sentence task had 20
  branch_only_at_root: true
  lr: 0.0001
  accumulate_grad_batches: 25
  epochs: 1 # next_sentence task had 300
  use_4bit: true 
  dojo_timeout: 60 # in seconds (LeanDojo's default is 600)
  ckpt_dest: "checkpoints"
  save_ckpt_on_val: true
  gradient_clip_val: 0.5 # default is 0 (no clipping)

  # validation loop frequency control
  # val_check_interval: float in [0, 1]: frequency per train epoch
  val_check_interval: 0.25
  # check_val_every_n_epoch: int (default=1)
  check_val_every_n_epoch: null

reward:
  temp_start: 1.0
  temp_end: 0.8
  temp_horizon: 750
  vocab_alpha: -50
  sentence_validator: null
  buffer_size: 50
  buffer_sim_tolerance: 0.25
  verifier_batch_size: 16 # TODO: figure out if we can go higher
  # TODO: get dsp adapters
  # reward_model_hf_id: "msho/llemma_sft"
  # reward_model_adapter_name: "reward"
  reward_model_hf_id: "msho/dspv1_sft"
  reward_model_adapter_name: "reward"
  # reward_model_hf_id: null
  # reward_model_adapter_name: null

callbacks:
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/logR"
    mode: "max"
    save_last: true
    dirpath: ${save_dir}/checkpoints/${now:%Y-%m-%d}_${now:%H-%M-%S}
    filename: "epoch={epoch:03d}"
    auto_insert_metric_name: true
  - _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/logR"
    mode: "max"
    patience: 10
  # next_sentence settings:
  # - _target_: pytorch_lightning.callbacks.ModelCheckpoint
  #   monitor: "val/logP(s) (avg)"
  #   mode: "max"
  #   save_last: true
  #   dirpath: ${save_dir}/checkpoints/${now:%Y-%m-%d}_${now:%H-%M-%S}
  #   filename: "epoch={epoch:03d}"
  #   auto_insert_metric_name: true
  # - _target_: pytorch_lightning.callbacks.EarlyStopping
  #   monitor: "val/logP(s) (avg)"
  #   mode: "max"
  #   patience: 10

constraints:
  max_tactics: 3
  min_tactic_tokens: 1
  max_tactic_tokens: 30

debug_logger:
  include_lean_dojo_debug: true
  log_debug_to_stdout: false
  write_to_file: true
  debug_log_file: "logs/debug1.log"
  
search_eval:
  probe_count: null # null for all
  sanity_check_probe_count: 20
  probe_file: data/val20.json
  
  search_params:
    _target_: proof_flow.src.search.common.ProofSearchParams
    num_sampled_tactics: 8
    timeout: 30
    max_expansions: null
    max_depth: 6
    num_workers: 1
    num_gpus: 1
    max_input_seq_len: 230 # 200 (state) + 30 (prompt)
    max_output_seq_len: 260
    max_new_tokens: 30
    length_penalty: 0.0

prompts:
  tac_gen: "ds_rm_st_v2"
