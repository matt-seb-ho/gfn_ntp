name: "ntp"

data:
  path: "data/time_filtered_v2.json"
  train_size: 0.95

model:
  # test with llemma, run later with dsp
  # name: "EleutherAI/llemma_7b"
  name: "deepseek-ai/DeepSeek-Prover-V1"

  bnb:
    _target_: transformers.BitsAndBytesConfig
    load_in_4bit: true
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: bfloat16

  lora_config:
    _target_: peft.LoraConfig

    # adjusted settings for OOM issues
    target_modules: "all-linear"
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"
    use_rslora: true

    # settings from SFT RM config
    # - https://www.philschmid.de/fine-tune-llms-in-2024-with-trl
    # target_modules: "all-linear"
    # r: 256
    # lora_alpha: 128
    # lora_dropout: 0.05
    # bias: "none"
    # task_type: "CAUSAL_LM"

    # next_sentence settings (gpt-2xl)
    # target_modules: ["c_attn", "c_proj", "c_fc"]
    # r: 64
    # lora_alpha: 16
    # lora_dropout: 0.1
    # bias: "none"
    # fan_in_fan_out: true
  
  # specify a huggingface model id to initialize the policy adapter from
  # e.g. "msho/llemma_sft"
  initialize_policy_adapter_from_pretrained: null

  # currently just used for replay
  inf_batch_size: 8

training:
  # subtb_lambda: 1.0
  pf_temp_high: 2.0
  pf_temp_low: 0.5
  pf_temp_prob: 0.666
  use_buffer_prob: 0.25
  n_samples: 8  # next_sentence task had 20
  lr: 0.0001
  accumulate_grad_batches: 25
  epochs: 1 # next_sentence task had 300
  use_4bit: true 
  dojo_timeout: 60 # in seconds (LeanDojo's default is 600)

eval:
  n_probes: 10
  diversity_metric: "sequence_embedding"

reward:
  temp_start: 1.0
  temp_end: 0.8
  temp_horizon: 750
  vocab_alpha: -50
  sentence_validator: null
  buffer_size: 50
  buffer_sim_tolerance: 0.25
  verifier_batch_size: 16 # TODO: figure out if we can go higher
  # TODO: get dsp adapters
  # reward_model_hf_id: "msho/llemma_sft"
  # reward_model_adapter_name: "reward"
  reward_model_hf_id: "msho/dspv1_sft_dspfmt"
  reward_model_adapter_name: "reward"
  # reward_model_hf_id: null
  # reward_model_adapter_name: null

callbacks:
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/logR"
    mode: "max"
    save_last: true
    dirpath: ${save_dir}/checkpoints/${now:%Y-%m-%d}_${now:%H-%M-%S}
    filename: "epoch={epoch:03d}"
    auto_insert_metric_name: true
  - _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/logR"
    mode: "max"
    patience: 10
  # next_sentence settings:
  # - _target_: pytorch_lightning.callbacks.ModelCheckpoint
  #   monitor: "val/logP(s) (avg)"
  #   mode: "max"
  #   save_last: true
  #   dirpath: ${save_dir}/checkpoints/${now:%Y-%m-%d}_${now:%H-%M-%S}
  #   filename: "epoch={epoch:03d}"
  #   auto_insert_metric_name: true
  # - _target_: pytorch_lightning.callbacks.EarlyStopping
  #   monitor: "val/logP(s) (avg)"
  #   mode: "max"
  #   patience: 10

constraints:
  max_tactics: 3
  min_tactic_tokens: 1
  max_tactic_tokens: 30
