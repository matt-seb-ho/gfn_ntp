paths:
  input_file: "data/time_filtered_val.json"
  search_eval_results_dir: "data/search_eval/"
  save_search_tree_dir: "data/sampled_search_trees"

use_tqdm: true
input_limit: null

reward_model:
  use_reward_model: false
  use_sts_format: false
  prompts_for_model: deepseek # {llemma, deepseek}
  normalize_length: true

model:
  # id: msho/dspv1_sft_dspfmt
  # id: kaiyuy/leandojo-lean4-tacgen-byt5-small
  id: deepseek-ai/DeepSeek-Prover-V1
  use_peft: false
  use_4bit: false

  quantization_config:
    _target_: transformers.BitsAndBytesConfig
    load_in_4bit: true
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: bfloat16

search:
  # tree seach settings
  num_sampled_tactics: 8
  timeout: 30
  max_expansions: null
  max_depth: 8
  save_results: true
  use_vllm: false

  # concurrency
  num_workers: 1
  num_gpus: 1

  # tactic generation settings
  # TODO: figure out max input/output tokens
  max_input_seq_len: 280
  # max_output_seq_len: 310

  # max tokens provenance: 
  # - v4 filtered to 250 state tokens
  # - using llemma rm st prompt gets to 280 tokens
  # - v5 filtered to have 30 new tokens
  # 1k filtering pulls down to 130 input
  # max_input_seq_len: 130
  max_output_seq_len: null
  max_new_tokens: 30
  length_penalty: 0.0

  # whether to save the trees
  save_search_trees: false
