# file paths --------------------------------------
# input_file: "data/time_filtered_val.json"
# output_file: "data/sampled_proofs.json"
# verification_results_file: "data/verification_results.json"
# rm_data_file: "data/rm_eval_dataset_v2.json"
# rm_eval_results_dir: "data/rm_eval_results"
# rm_eval_result_filename: "dsp1_sts_llprompt.json"
# use_tqdm: true
# input_limit: null

# Negative Tactics Data Generation: training split not test split -----------------------
# input_file: "/home/vincentzhu/gfn_ntp/data/time_filtered_v3_wfs_30_samples.json"  # Updated to use training data split
# output_file: "data/sampled_proofs_BFS.json"  # Updated output file for sampled proofs
# verification_results_file: "data/verification_results_BFS.json"  # Updated for training verification results
# rm_data_file: "data/negative_tactics_dataset_BFS.json"  # Updated RM data output file for training
# use_tqdm: true
# input_limit: null
# use_BFS: true

input_file: "data/time_filtered_val.json"
output_file: "data/sampled_proofs.json"
verification_results_file: "data/verification_results.json"
rm_data_file: "data/rm_eval_dataset_v3.json"
rm_eval_results_dir: "data/rm_eval_results/valid_tacs/"
rm_eval_result_filename: "peft_sancheck.json"
use_tqdm: true
input_limit: null
valid_tactics_only: true

# selecting pairs to evaluate on -------------------
# eval pairs {first, random, all}
pair_selection_strategy: first
max_pairs_per_state: 1
# pair_selection_strategy: random 
# max_pairs_per_state: 5

# RM scoring settings -----------------------------
use_sts_format: false
prompts_for_model: deepseek # {llemma, deepseek}
normalize_length: true


# model config  ------------------------------------
# model: deepseek-ai/DeepSeek-Prover-V1
# deepseek note:
# - debugging! asked on email & discord
# - currently trying llama arch (to match math base model)

# model: EleutherAI/llemma_7b
# model: internlm/internlm2-math-base-7b
# use_peft: false

# trained verifiers
# model: msho/llemma_sft
model: msho/dspv1_sft_dspfmt
# model: msho/dspv1_sft
use_peft: true


# generating evaluation data -----------------------
# values from DeepSeek-Prover-V1.5/configs/sampling.py
# initialization from DeepSeek-Prover-V1.5/prover/workers/generator.py
# - except seed which was `seed = int(time.time()) % 1000 + (self.node_rank * 8 + self.local_rank) * 1000`
data_generation:
  llm:
    model: deepseek-ai/DeepSeek-Prover-V1.5-RL
    max_num_batched_tokens: 8192
    seed: 42
    trust_remote_code: true
  device: "cuda"
  max_inp_seq_len: 256
  max_oup_seq_len: 30
  length_penalty: 1.0
  template: "%s"
  max_expansions: 10
  timeout: 3000

  sampling_params:
    temperature: 1
    max_tokens: 2048
    top_p: 0.95
    n: 8
