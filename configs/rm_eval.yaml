# file paths --------------------------------------
# input_file: "data/time_filtered_val.json"
# output_file: "data/sampled_proofs.json"
# verification_results_file: "data/verification_results.json"
# rm_data_file: "data/rm_eval_dataset_v2.json"
# rm_eval_results_dir: "data/rm_eval_results"
# rm_eval_result_filename: "dsp1_sts_llprompt.json"
# use_tqdm: true
# input_limit: null

# Negative Tactics Data Generation: training split not test split -----------------------
# input_file: "data/straight_shot_proof_sample/time_filtered_v2_wfs.json"  # Updated to use training data split
# output_file: "data/straight_shot_proof_sample/sampled_proofs_train.json"  # Updated output file for sampled proofs
# verification_results_file: "data/straight_shot_proof_sample/verification_results_train.json"  # Updated for training verification results
# rm_data_file: "data/rm_eval_dataset_v3.json"  # Updated RM data output file for training
# rm_eval_results_dir: "data/rm_eval_results/"
# rm_eval_result_filename: "dspv1_dpo_dspfmt_rm_eval_all.json"
# use_tqdm: true
# input_limit: null
# valid_tactics_only: true
# use_BFS: false

input_file: "data/straight_shot_proof_sample/time_filtered_v2_wfs.json"  # Updated to use training data split
output_file: "data/straight_shot_proof_sample/sampled_proofs_train.json"  # Updated output file for sampled proofs
verification_results_file: "data/straight_shot_proof_sample/verification_results_train.json"  # Updated for training verification results
rm_data_file: "data/rm_eval_dataset_v3.json"  # Updated RM data output file for training
rm_eval_results_dir: "data/rm_eval_results/"
rm_eval_result_filename: "dspv1_dpo_dspfmt_rm_eval_all.json"
use_tqdm: true
input_limit: null
valid_tactics_only: true
use_BFS: false

# input_file: "data/time_filtered_val.json"
# output_file: "data/sampled_proofs.json"
# verification_results_file: "data/verification_results.json"
# rm_data_file: "data/rm_eval_dataset_v3.json"
# rm_eval_results_dir: "data/rm_eval_results/valid_tacs/"
# rm_eval_result_filename: "peft_sancheck.json"
# use_tqdm: true
# input_limit: null
# valid_tactics_only: true

# selecting pairs to evaluate on -------------------
# eval pairs {first, random, all}
# pair_selection_strategy: first
max_pairs_per_state: 1
pair_selection_strategy: all 
# max_pairs_per_state: 5

# RM scoring settings -----------------------------
use_sts_format: false
prompts_for_model: deepseek # {llemma, deepseek}
normalize_length: true


# model config  ------------------------------------
# model: deepseek-ai/DeepSeek-Prover-V1
# deepseek note:
# - debugging! asked on email & discord
# - currently trying llama arch (to match math base model)

# model: EleutherAI/llemma_7b
# model: internlm/internlm2-math-base-7b
# use_peft: false

# trained verifiers
# model: msho/llemma_sft
# model: msho/dspv1_sft
# model: vincentlinzhu/dspv1_dpo_dspfmt
model: vincentlinzhu/dspv1_dpo_dspfmt_medium
# model: vincentlinzhu/dspv1_dpo_llemmafmt_medium
use_peft: true
device: cuda

# generating evaluation data -----------------------
# values from DeepSeek-Prover-V1.5/configs/sampling.py
# initialization from DeepSeek-Prover-V1.5/prover/workers/generator.py
# - except seed which was `seed = int(time.time()) % 1000 + (self.node_rank * 8 + self.local_rank) * 1000`
data_generation:
  llm:
    model: deepseek-ai/DeepSeek-Prover-V1.5-RL
    max_num_batched_tokens: 8192
    seed: 42
    trust_remote_code: true

  # max_inp_seq_len: 256
  # max_oup_seq_len: 30
  # length_penalty: 0.0
  # template: "%s"
  # max_expansions: 50
  # timeout: 3000

  sampling_params:
    temperature: 1
    max_tokens: 2048
    top_p: 0.95
    n: 8
