{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Lean Verifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download Model\n",
    "We discussed 2 candidate models:\n",
    "1. [LeanDojo's ReProver*](https://github.com/lean-dojo/ReProver?tab=readme-ov-file#using-trained-models-on-hugging-face)\n",
    "2. [Llemma (LM for Math)](https://huggingface.co/EleutherAI/llemma_7b)\n",
    "\n",
    "*we probably want the Lean 4 + Retiever-Less version: [hf card](https://huggingface.co/kaiyuy/leandojo-lean4-tacgen-byt5-small)\n",
    "\n",
    "ReProver is trained specially on Lean data, while Llemma is trained on various math texts, including proof assistant code (Lean, Isabelle).\n",
    "ReProver however is a ByT5-small derivative, so it only has 300M parameters. \n",
    "In comparison, Llemma is 7B model (based on CodeLlama).\n",
    "\n",
    "In the long term we probably want a richer 7B model to optimize against.\n",
    "But for initial experiments I'll start with ReProver for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from time import perf_counter\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM, # Llemma\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5640ba452f42eda800e722c639a4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d6686de7e840b084a0bca94f00c0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/3.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b86f94eae0a4c58a2e5c524b7a112df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687730f9a4eb449da3f3bf3eeaaf5111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the retriever-less version\n",
    "# - can switch \"lean4 <-> lean3\"\n",
    "reprover_model_path = \"kaiyuy/leandojo-lean4-tacgen-byt5-small\"\n",
    "reprover_tokenizer = AutoTokenizer.from_pretrained(reprover_model_path)\n",
    "reprover = AutoModelForSeq2SeqLM.from_pretrained(reprover_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tactic code example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = \"n : ℕ\\n⊢ gcd n n = n\"\n",
    "tokenized_state = reprover_tokenizer(state, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a single tactic.\n",
    "tactic_ids = reprover.generate(tokenized_state.input_ids, max_length=1024)\n",
    "tactic = reprover_tokenizer.decode(tactic_ids[0], skip_special_tokens=True)\n",
    "print(tactic, end=\"\\n\\n\")\n",
    "\n",
    "# Generate multiple tactics via beam search.\n",
    "# add timing code to get a feel for latency\n",
    "_start = perf_counter()\n",
    "tactic_candidates_ids = reprover.generate(\n",
    "    tokenized_state.input_ids,\n",
    "    max_length=1024,\n",
    "    num_beams=4,\n",
    "    length_penalty=0.0,\n",
    "    do_sample=False,\n",
    "    num_return_sequences=4,\n",
    "    early_stopping=False,\n",
    ")\n",
    "print(f\"Generated 4 candidates via beam search in {perf_counter() - _start}s\")\n",
    "tactic_candidates = reprover_tokenizer.batch_decode(\n",
    "    tactic_candidates_ids, skip_special_tokens=True\n",
    ")\n",
    "for tac in tactic_candidates:\n",
    "    print(tac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prep Data\n",
    "LeanDojo's (lean4) dataset is hosted [here](https://zenodo.org/records/10114185).\n",
    "The tar file is also copied in this repo (`REPO_ROOT/data/leandojo_benchmark_4_v5.tar.gz`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility\n",
    "REPO_ROOT = \"/mnt/hdd/msho/gfn_ntp/\"\n",
    "def full_path(path_relative_to_project_repo_root):\n",
    "    return os.path.join(REPO_ROOT, path_relative_to_project_repo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's unpack the tar file\n",
    "import tarfile\n",
    "data_dir = full_path(\"data/\")\n",
    "tar_path = os.path.join(REPO_ROOT, \"data/leandojo_benchmark_4_v5.tar.gz\")\n",
    "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qlora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
