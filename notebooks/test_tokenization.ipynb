{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_MODEL_ID = \"EleutherAI/llemma_7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check special tokens\n",
    "\n",
    "def check_special_tokens(tokenizer):\n",
    "    print(\"Start of sequence token:\", tokenizer.bos_token)\n",
    "    print(\"End of sequence token:\", tokenizer.eos_token)\n",
    "    print(\"Padding token:\", tokenizer.pad_token)\n",
    "    print(\"Unknown token:\", tokenizer.unk_token)\n",
    "    print(\"Mask token:\", tokenizer.mask_token)\n",
    "\n",
    "    # Listing all special tokens\n",
    "    print(\"All special tokens:\", tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of sequence token: <s>\n",
      "End of sequence token: </s>\n",
      "Padding token: None\n",
      "Unknown token: <unk>\n",
      "Mask token: None\n",
      "All special tokens: ['<s>', '</s>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "check_special_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = [\n",
    "    \"\\n\",\n",
    "    \"hello.\",\n",
    "    \"hello.\\n\",\n",
    "    \"hello.\\nworld.\",\n",
    "    \"\\n\\n\",\n",
    "    \"hello.\\n\\n\",\n",
    "    \"hello.\\n\\nworld.\",\n",
    "    \"different prefix\\n\",\n",
    "    \"different prefix\\ndifferent suffix\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 22172, 29889, 13], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0 = test_strings[2]\n",
    "res0 = tokenizer(s0)\n",
    "res0.char_to_token(0)\n",
    "res0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tokenization(tokenizer, test_string, token='\\n'):\n",
    "    encoded = tokenizer(test_string)\n",
    "    input_ids = encoded['input_ids']\n",
    "    decoded_ids = [tokenizer.decode(i) for i in input_ids]\n",
    "    print((\n",
    "        f\"original string: {repr(test_string)}\\n\"\n",
    "        f\"- encoded: {input_ids}\\n\"\n",
    "        f\"- decoded: {decoded_ids}\\n\"\n",
    "        f\"target token: {repr(token)}\\n\"\n",
    "        f\"- encoded alone: {tokenizer.encode(token, add_special_tokens=False)}\"\n",
    "    ))\n",
    "    char_idx = test_string.find(token)\n",
    "    if char_idx == -1:\n",
    "        print(\"- target token not found in test string\")\n",
    "        return\n",
    "    token_idx = encoded.char_to_token(char_idx)\n",
    "    print((\n",
    "        f\"- {repr(token)} char idx: {repr(char_idx)}\\n\"\n",
    "        f\"- {repr(token)} token idx: {repr(token_idx)}\\n\"\n",
    "        f\"- {repr(token)} token id: {repr(input_ids[token_idx])}\\n\"\n",
    "        f\"- {repr(token)} decoded: {repr(tokenizer.decode(input_ids[token_idx]))}\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedirectStdoutToFile:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.original_stdout = sys.stdout  # Save the original stdout\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.file = open(self.filepath, 'w')\n",
    "        sys.stdout = self.file\n",
    "        return self  # This can be omitted if not using the as variable\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout = self.original_stdout  # Restore stdout\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with RedirectStdoutToFile(\"dummy.txt\") as f:\n",
    "    for test_string in test_strings:\n",
    "        check_tokenization(tokenizer, test_string)\n",
    "        print(\"###\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
